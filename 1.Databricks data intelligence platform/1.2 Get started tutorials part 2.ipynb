{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f1d43b7-1338-4310-aef6-42cea5b24a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tutorial: Build an ETL pipeline with Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "This tutorial explains how to create and deploy an ETL (extract, transform, and load) pipeline for data orchestration using Lakeflow Spark Declarative Pipelines and Auto Loader. An ETL pipeline implements the steps to read data from source systems, transform that data based on requirements, such as data quality checks and record de-duplication, and write the data to a target system, such as a data warehouse or a data lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31a33887-c5b5-440d-a7b8-ca5e4d7a118c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, create an pipeline by defining the datasets in files (called source code) using pipeline syntax. Each source code file can contain only one language, but you can add multiple language-specific files in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee77555-25e3-4ceb-8d88-a0af45785d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here is the full path:\n",
    "\n",
    "/Workspace/Users/yesmanki81@gmail.com/Databricks_Basic/1.Databricks data intelligence platform/ETL Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c08f9d2-c226-4639-9a22-1f5b9a58d86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructType, StructField\n",
    "\n",
    "# Define the path to the source data\n",
    "file_path = f\"/databricks-datasets/songs/data-001/\"\n",
    "\n",
    "# Define a streaming table to ingest data from a volume\n",
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"artist_id\", StringType(), True),\n",
    "    StructField(\"artist_lat\", DoubleType(), True),\n",
    "    StructField(\"artist_long\", DoubleType(), True),\n",
    "    StructField(\"artist_location\", StringType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"duration\", DoubleType(), True),\n",
    "    StructField(\"end_of_fade_in\", DoubleType(), True),\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"key_confidence\", DoubleType(), True),\n",
    "    StructField(\"loudness\", DoubleType(), True),\n",
    "    StructField(\"release\", StringType(), True),\n",
    "    StructField(\"song_hotnes\", DoubleType(), True),\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"start_of_fade_out\", DoubleType(), True),\n",
    "    StructField(\"tempo\", DoubleType(), True),\n",
    "    StructField(\"time_signature\", DoubleType(), True),\n",
    "    StructField(\"time_signature_confidence\", DoubleType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"partial_sequence\", IntegerType(), True)\n",
    "  ]\n",
    ")\n",
    "\n",
    "@dp.table(\n",
    "  comment=\"Raw data from a subset of the Million Song Dataset; a collection of features and metadata for contemporary music tracks.\"\n",
    ")\n",
    "def songs_raw():\n",
    "  return (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .schema(schema)\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"sep\",\"\\t\")\n",
    "    .load(file_path))\n",
    "\n",
    "# Define a materialized view that validates data and renames a column\n",
    "@dp.materialized_view(\n",
    "  comment=\"Million Song Dataset with data cleaned and prepared for analysis.\"\n",
    ")\n",
    "@dp.expect(\"valid_artist_name\", \"artist_name IS NOT NULL\")\n",
    "@dp.expect(\"valid_title\", \"song_title IS NOT NULL\")\n",
    "@dp.expect(\"valid_duration\", \"duration > 0\")\n",
    "def songs_prepared():\n",
    "  return (\n",
    "    spark.read.table(\"songs_raw\")\n",
    "      .withColumnRenamed(\"title\", \"song_title\")\n",
    "      .select(\"artist_id\", \"artist_name\", \"duration\", \"release\", \"tempo\", \"time_signature\", \"song_title\", \"year\")\n",
    "  )\n",
    "\n",
    "# Define a materialized view that has a filtered, aggregated, and sorted view of the data\n",
    "@dp.materialized_view(\n",
    "  comment=\"A table summarizing counts of songs released by the artists who released the most songs each year.\"\n",
    ")\n",
    "def top_artists_by_year():\n",
    "  return (\n",
    "    spark.read.table(\"songs_prepared\")\n",
    "      .filter(expr(\"year > 0\"))\n",
    "      .groupBy(\"artist_name\", \"year\")\n",
    "      .count().withColumnRenamed(\"count\", \"total_number_of_songs\")\n",
    "      .sort(desc(\"total_number_of_songs\"), desc(\"year\"))\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04c68ed1-2b58-41db-9ee8-2d34893251bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Define a streaming table to ingest data from a volume\n",
    "CREATE OR REFRESH STREAMING TABLE songs_raw\n",
    "COMMENT \"Raw data from a subset of the Million Song Dataset; a collection of features and metadata for contemporary music tracks.\"\n",
    "AS SELECT *\n",
    "FROM STREAM read_files(\n",
    "  '/databricks-datasets/songs/data-001/part*',\n",
    "  format => \"csv\",\n",
    "  header => \"false\",\n",
    "  delimiter => \"\\t\",\n",
    "  schema => \"\"\"\n",
    "    artist_id STRING,\n",
    "    artist_lat DOUBLE,\n",
    "    artist_long DOUBLE,\n",
    "    artist_location STRING,\n",
    "    artist_name STRING,\n",
    "    duration DOUBLE,\n",
    "    end_of_fade_in DOUBLE,\n",
    "    key INT,\n",
    "    key_confidence DOUBLE,\n",
    "    loudness DOUBLE,\n",
    "    release STRING,\n",
    "    song_hotnes DOUBLE,\n",
    "    song_id STRING,\n",
    "    start_of_fade_out DOUBLE,\n",
    "    tempo DOUBLE,\n",
    "    time_signature INT,\n",
    "    time_signature_confidence DOUBLE,\n",
    "    title STRING,\n",
    "    year INT,\n",
    "    partial_sequence STRING\n",
    "  \"\"\",\n",
    "  schemaEvolutionMode => \"none\");\n",
    "\n",
    "-- Define a materialized view that validates data and renames a column\n",
    "CREATE OR REFRESH MATERIALIZED VIEW songs_prepared(\n",
    "CONSTRAINT valid_artist_name EXPECT (artist_name IS NOT NULL),\n",
    "CONSTRAINT valid_title EXPECT (song_title IS NOT NULL),\n",
    "CONSTRAINT valid_duration EXPECT (duration > 0)\n",
    ")\n",
    "COMMENT \"Million Song Dataset with data cleaned and prepared for analysis.\"\n",
    "AS SELECT artist_id, artist_name, duration, release, tempo, time_signature, title AS song_title, year\n",
    "FROM songs_raw;\n",
    "\n",
    "-- Define a materialized view that has a filtered, aggregated, and sorted view of the data\n",
    "CREATE OR REFRESH MATERIALIZED VIEW top_artists_by_year\n",
    "COMMENT \"A table summarizing counts of songs released by the artists each year, who released the most songs.\"\n",
    "AS SELECT\n",
    "  artist_name,\n",
    "  year,\n",
    "  COUNT(*) AS total_number_of_songs\n",
    "FROM songs_prepared\n",
    "WHERE year > 0\n",
    "GROUP BY artist_name, year\n",
    "ORDER BY total_number_of_songs DESC, year DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a816e420-7afd-4b60-a9f4-7d0ca31461b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this step, you perform ad-hoc queries on the data processed in the ETL pipeline to analyze the song data in the Databricks SQL Editor. These queries use the prepared records created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3359b9f-3b52-4cb4-af90-f7e9ff576840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Which artists released the most songs each year in 1990 or later?\n",
    "SELECT artist_name, total_number_of_songs, year\n",
    "  -- replace with the catalog/schema you are using:\n",
    "  FROM <catalog>.<schema>.top_artists_by_year\n",
    "  WHERE year >= 1990\n",
    "  ORDER BY total_number_of_songs DESC, year DESC;\n",
    "\n",
    "\n",
    "-- Find songs with a 4/4 beat and danceable tempo\n",
    "SELECT artist_name, song_title, tempo\n",
    "  -- replace with the catalog/schema you are using:\n",
    "  FROM <catalog>.<schema>.songs_prepared\n",
    "  WHERE time_signature = 4 AND tempo between 100 and 140;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4456f6d1-443b-4c1d-a6c1-8142a861ad98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tutorial: Build an ETL pipeline with Apache Spark on the Databricks platform\n",
    "This tutorial shows you how to develop and deploy your first ETL (extract, transform, and load) pipeline for data orchestration with Apache Spark. Although this tutorial uses Databricks all-purpose compute, you can also use serverless compute if it's enabled for your workspace.\n",
    "\n",
    "You can also use Lakeflow Spark Declarative Pipelines to build ETL pipelines. Databricks Lakeflow Spark Declarative Pipelines reduces the complexity of building, deploying, and maintaining production ETL pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3119e1a-1dfa-4f36-8da7-c815cf677c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To do exploratory data analysis and data engineering, create a cluster to provide the compute resources needed to execute commands.\n",
    "\n",
    "To create a notebook in your workspace, click New Icon New in the sidebar, and then click Notebook. A blank notebook opens in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eee76b20-b8fa-4c8a-ae81-bbd88b6a99b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files = dbutils.fs.ls('/databricks-datasets/structured-streaming/events')\n",
    "display(files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe34d512-764a-456b-a938-eedd9c84e656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head('dbfs:/databricks-datasets/structured-streaming/events/file-0.json', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9475278-5efc-44c8-8456-25a0fa2a9be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\n",
    "    \"dbfs:/databricks-datasets/structured-streaming/events/\"\n",
    ")\n",
    "\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebf1d13e-d9e5-4087-bf8f-c6a5c0ddfca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.2 Get started tutorials part 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
